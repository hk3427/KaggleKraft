{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21733,"databundleVersionId":1408234,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Contradictory, My Dear Watson\n\nCan machines determine the relationships between sentences?\n\nGiven two sentences, there are three ways they could be related:\n* one could entail the other\n* one could contradict the other\n* they could be unrelated","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom datasets import Dataset\nfrom transformers import AdamW, pipeline\n\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-09-27T12:25:24.371486Z","iopub.execute_input":"2024-09-27T12:25:24.371830Z","iopub.status.idle":"2024-09-27T12:25:43.184908Z","shell.execute_reply.started":"2024-09-27T12:25:24.371795Z","shell.execute_reply":"2024-09-27T12:25:43.184113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the data","metadata":{}},{"cell_type":"code","source":"train_raw = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ntest = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')\n\ntrain_raw\\\n    .loc[lambda df: df.lang_abv.eq('en')]\\\n    .groupby('label')\\\n    .sample(2, random_state=1)\\\n    [['premise', 'hypothesis', 'label']]\\\n    .style.hide(axis='index')","metadata":{"execution":{"iopub.status.busy":"2024-09-27T12:25:49.608468Z","iopub.execute_input":"2024-09-27T12:25:49.609157Z","iopub.status.idle":"2024-09-27T12:25:49.866329Z","shell.execute_reply.started":"2024-09-27T12:25:49.609119Z","shell.execute_reply":"2024-09-27T12:25:49.865418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Translation Augmentation","metadata":{}},{"cell_type":"code","source":"def translate_back_and_forth(text):\n    \"\"\"Translate English text to French and then French back to English.\"\"\"\n    \n     \n    device = 0 if torch.cuda.is_available() else -1\n    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Load French/English translation models\n    translator_en_to_fr = pipeline(\"translation\",\n                                   model=\"Helsinki-NLP/opus-mt-en-fr\",\n                                   device=device)\n    translator_fr_to_en = pipeline(\"translation\",\n                                   model=\"Helsinki-NLP/opus-mt-fr-en\",\n                                   device=device)\n\n    # Translate from English to French\n    translated_text = translator_en_to_fr(text)[0]['translation_text']\n\n    # Translate back from French to English\n    back_translated_text = translator_fr_to_en(translated_text)[0]['translation_text']\n\n    return back_translated_text\n\noriginal_text = \"The quick brown fox jumps over the lazy dog.\"\naugmented_text = translate_back_and_forth(original_text)\nprint(f\"Original: {original_text}\")\nprint(f\"Augmented: {augmented_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T12:26:58.982470Z","iopub.execute_input":"2024-09-27T12:26:58.983190Z","iopub.status.idle":"2024-09-27T12:27:16.102484Z","shell.execute_reply.started":"2024-09-27T12:26:58.983150Z","shell.execute_reply":"2024-09-27T12:27:16.101499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_raw\\\n    .head()\\\n    .assign(\n        premise_aug = lambda df: np.select(\n            [df.lang_abv.ne('en')],\n            [df.premise],\n            df.premise.apply(translate_back_and_forth)\n        ),\n        hypothesis_aug = lambda df: np.select(\n            [df.lang_abv.ne('en')],\n            [df.hypothesis],\n            df.hypothesis.apply(translate_back_and_forth)\n        )\n    )\\\n    [['premise', 'premise_aug', 'hypothesis', 'hypothesis_aug']]\\\n    .style.hide(axis='index')","metadata":{"execution":{"iopub.status.busy":"2024-09-27T12:27:19.176914Z","iopub.execute_input":"2024-09-27T12:27:19.177306Z","iopub.status.idle":"2024-09-27T12:28:19.117492Z","shell.execute_reply.started":"2024-09-27T12:27:19.177268Z","shell.execute_reply":"2024-09-27T12:28:19.116504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a GPU compatitble version of the above functionality","metadata":{}},{"cell_type":"code","source":"def translate_back_and_forth_batch(texts):\n    \"\"\"Translate a batch of English text to French and back to English.\"\"\"\n    \n    # Check if CUDA is available, otherwise default to CPU\n    device = 0 if torch.cuda.is_available() else -1\n\n    # Load French/English translation models\n    translator_en_to_fr = pipeline(\"translation\", \n                                   model=\"Helsinki-NLP/opus-mt-en-fr\", \n                                   device=device)\n    translator_fr_to_en = pipeline(\"translation\", \n                                   model=\"Helsinki-NLP/opus-mt-fr-en\", \n                                   device=device)\n\n    # Translate from English to French\n    translated_texts = translator_en_to_fr(texts)\n    translated_texts = [t['translation_text'] for t in translated_texts]\n\n    # Translate back from French to English\n    back_translated_texts = translator_fr_to_en(translated_texts)\n    back_translated_texts = [t['translation_text'] for t in back_translated_texts]\n\n    return back_translated_texts\n\noriginal_text = \"The quick brown fox jumps over the lazy dog.\"\naugmented_text = translate_back_and_forth_batch(original_text)\nprint(f\"Original: {original_text}\")\nprint(f\"Augmented: {augmented_text[0]}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T12:29:29.431859Z","iopub.execute_input":"2024-09-27T12:29:29.432243Z","iopub.status.idle":"2024-09-27T12:29:34.665547Z","shell.execute_reply.started":"2024-09-27T12:29:29.432204Z","shell.execute_reply":"2024-09-27T12:29:34.664454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef aug_with_fr_en(input_df):\n    \"\"\"Augment dataset with English->French->English alternatives.\"\"\"\n    \n    # Filter only English text\n    en_df = input_df.loc[input_df.lang_abv.eq('en')]\n    \n    # Translate 'premise' column in batches\n    premise_texts = en_df['premise'].tolist()\n    premise_translations = translate_back_and_forth_batch(premise_texts)\n    \n    # Translate 'hypothesis' column in batches\n    hypothesis_texts = en_df['hypothesis'].tolist()\n    hypothesis_translations = translate_back_and_forth_batch(hypothesis_texts)\n    \n    # Concatenate original and augmented data\n    return pd.concat([input_df,\n                      en_df\\\n                         .assign(\n                              premise=premise_translations,\n                              hypothesis=hypothesis_translations\n                      )])\n\ntrain_aug = aug_with_fr_en(train_raw)\\\n    .drop_duplicates(subset=['premise', 'hypothesis'])\\\n    .reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T12:30:09.682711Z","iopub.execute_input":"2024-09-27T12:30:09.683410Z","iopub.status.idle":"2024-09-27T12:30:21.570070Z","shell.execute_reply.started":"2024-09-27T12:30:09.683362Z","shell.execute_reply":"2024-09-27T12:30:21.568962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the PyTorch `Dataset`","metadata":{}},{"cell_type":"code","source":"train_hf = Dataset.from_pandas(train_aug)\ntest_hf = Dataset.from_pandas(test)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T12:03:41.619054Z","iopub.execute_input":"2024-09-27T12:03:41.619911Z","iopub.status.idle":"2024-09-27T12:03:41.667404Z","shell.execute_reply.started":"2024-09-27T12:03:41.619871Z","shell.execute_reply":"2024-09-27T12:03:41.666194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"# Load the tokenizer for BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\n# Tokenization function\ndef tokenize_function(examples):\n    return tokenizer(examples['premise'],\n                     examples['hypothesis'],\n                     padding='max_length',\n                     truncation=True)\n\n# Apply tokenization\ntrain_encoded = train_hf.map(tokenize_function, batched=True).map(\n    lambda examples: {'labels': examples['label']},\n    batched=True\n)\ntest_encoded = test_hf.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T08:49:21.172903Z","iopub.execute_input":"2024-09-27T08:49:21.173630Z","iopub.status.idle":"2024-09-27T08:49:55.165398Z","shell.execute_reply.started":"2024-09-27T08:49:21.173588Z","shell.execute_reply":"2024-09-27T08:49:55.164220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\ntest_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask'])","metadata":{"execution":{"iopub.status.busy":"2024-09-27T08:50:04.605181Z","iopub.execute_input":"2024-09-27T08:50:04.606131Z","iopub.status.idle":"2024-09-27T08:50:04.612597Z","shell.execute_reply.started":"2024-09-27T08:50:04.606091Z","shell.execute_reply":"2024-09-27T08:50:04.611436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create `DataLoaders` for training and testing","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train_encoded, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_encoded, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T08:50:15.404221Z","iopub.execute_input":"2024-09-27T08:50:15.404684Z","iopub.status.idle":"2024-09-27T08:50:15.410759Z","shell.execute_reply.started":"2024-09-27T08:50:15.404639Z","shell.execute_reply":"2024-09-27T08:50:15.409283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"# Load BERT model for sequence classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)\n\n# Move the model to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Set up the optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Training loop\nmodel.train()\nfor epoch in range(3):  # 3 epochs for this example\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        optimizer.zero_grad()\n\n        # Move batch to GPU\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        # Forward pass\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n\n        # Update weights\n        optimizer.step()\n\n        # Print loss\n        loop.set_description(f'Epoch {epoch}')\n        loop.set_postfix(loss=loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-09-27T08:50:47.462416Z","iopub.execute_input":"2024-09-27T08:50:47.462856Z","iopub.status.idle":"2024-09-27T09:45:13.556486Z","shell.execute_reply.started":"2024-09-27T08:50:47.462815Z","shell.execute_reply":"2024-09-27T09:45:13.555288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Predictions on Test Dataset","metadata":{}},{"cell_type":"code","source":"# Set model to eval mode\nmodel.eval()\n\n# Create an empty list to store predictions\npredictions = []\n\n# Disable gradient calculation for inference\nwith torch.no_grad():  \n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        # Get model outputs (logits) for the batch\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        # Convert logits to predicted labels (class with the highest score)\n        predicted_labels = torch.argmax(logits, dim=-1)\n\n        # Move predictions back to CPU and append to the list\n        predictions.extend(predicted_labels.cpu().numpy())\n\n\ntest_with_preds = test\\\n    .assign(\n        prediction = predictions\n    )\n\ntest_with_preds\\\n    [['id', 'prediction']]\\\n    .to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T09:45:26.472374Z","iopub.execute_input":"2024-09-27T09:45:26.473094Z","iopub.status.idle":"2024-09-27T09:47:59.805336Z","shell.execute_reply.started":"2024-09-27T09:45:26.473052Z","shell.execute_reply":"2024-09-27T09:47:59.804459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Translation Experiment\n\n\n```python\ndef translate_back_and_forth(text, tf=False):\n    \"\"\"Translate English text to French and then French back to English.\"\"\"\n    \n    if tf:\n        return text\n    \n    if not tf:\n        from transformers import pipeline\n        \n        # Load French/English translation models\n        translator_en_to_fr = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n        translator_fr_to_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n\n        # Translate from English to French\n        translated_text = translator_en_to_fr(text)[0]['translation_text']\n\n        # Translate back from French to English\n        back_translated_text = translator_fr_to_en(translated_text)[0]['translation_text']\n\n        return back_translated_text\n\n# Example usage\noriginal_text = \"The quick brown fox jumps over the lazy dog.\"\naugmented_text = translate_back_and_forth(original_text)\nprint(f\"Original: {original_text}\")\nprint(f\"Augmented: {augmented_text}\")\n\ntrain_raw\\\n    .head()\\\n    .assign(\n        premise_aug = lambda df: np.select(\n            [df.lang_abv.ne('en')],\n            [df.premise],\n            df.premise.apply(translate_back_and_forth)\n        ),\n        hypothesis_aug = lambda df: np.select(\n            [df.lang_abv.ne('en')],\n            [df.hypothesis],\n            df.hypothesis.apply(translate_back_and_forth)\n        )\n    )\\\n    [['premise', 'premise_aug', 'hypothesis', 'hypothesis_aug']]\\\n    .style.hide(axis='index')\n\ndef aug_with_fr_en(input_df):\n    \"\"\"Augment dataset with English->French->English alternatives.\"\"\"\n    return pd.concat([\n        input_df,\n        input_df\\\n            .loc[lambda df: df.lang_abv.eq('en')]\\\n            .assign(\n                premise = lambda df: np.select(\n                    [df.lang_abv.ne('en')],\n                    [df.premise],\n                    df.premise.apply(translate_back_and_forth)\n                ),\n                hypothesis = lambda df: np.select(\n                    [df.lang_abv.ne('en')],\n                    [df.hypothesis],\n                    df.hypothesis.apply(translate_back_and_forth)\n                )\n            )\\\n        ])\n\ntrain_aug = aug_with_fr_en(train_raw)\\\n    .drop_duplicates(subset=['premise', 'hypothesis'])\\\n    .reset_index(drop=True)\n\n```\n\nDoes not work on TPU.","metadata":{}}]}